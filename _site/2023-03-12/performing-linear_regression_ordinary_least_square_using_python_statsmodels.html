<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>M.A. Portfolio Website</title>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  
  <!-- Font Awesome -->
  <link rel="stylesheet" type="text/css" href="/assets/css/fontawesome-all.min.css">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon.ico">

  <!-- Google Analytics -->
  
  <script>
      (function(i, s, o, g, r, a, m) {
          i['GoogleAnalyticsObject'] = r;
          i[r] = i[r] || function() {
              (i[r].q = i[r].q || []).push(arguments)
          }, i[r].l = 1 * new Date();
          a = s.createElement(o),
              m = s.getElementsByTagName(o)[0];
          a.async = 1;
          a.src = g;
          m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

      ga('create', 'G-NKY0QRYQND', 'auto');
      ga('send', 'pageview');

  </script>
  

</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NKY0QRYQND"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NKY0QRYQND');
</script>


  <body>
    <nav class="nav">
      <div class="nav-container">
        <a href="/">
          <h2 class="nav-title">M.A. Portfolio Website</h2>
        </a>
        <ul>
          <li><a href="/">About</a></li>
          <li><a href="/portfolio/">Portfolio</a></li>
        </ul>
    </div>
  </nav>

    <main>
      <div class="post">
  <h2 class="post-title">Performing Linear Regression Analysis (Ordinary Least Square) Using Python Statsmodels</h2>
  <div class="post-line"></div>

  <p>The objective of this project is to perform linear regression analysis (ordinary least square technique) using Python Statsmodels to predict the car price, based on the automobile dataset from <a href="https://archive.ics.uci.edu/ml/datasets/automobile">UCI Machine Learning repository</a>, which is a common dataset for regression analysis. The automobile dataset is from the year 1985 which is quite old, but it’s suitable for the learning purposes of this project. You can find the project’s jupyter notebook and the dataset (if you want to skip extracting step) on my <a href="https://github.com/mohammad-agus/linear_regression_ordinary_least_square">GitHub repository</a>.</p>

<h3 id="import-library">Import Library</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># download dataset description from the source
</span><span class="kn">from</span> <span class="nn">urllib</span> <span class="kn">import</span> <span class="n">request</span> 

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="data-preprocessing--exploration">Data Preprocessing &amp; Exploration</h1>

<h3 id="extract-the-dataset">Extract the Dataset</h3>

<ul>
  <li>Download and read the dataset description.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_desc_url</span> <span class="o">=</span> <span class="s">"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names"</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">data_desc_url</span><span class="p">,</span> <span class="s">"dataset/data_desc.names"</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"dataset/data_desc.names"</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_desc</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">data_desc</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=6&amp;hideInput=true" title="Jovian Viewer" height="800" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Read the dataset and assign the column header.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># url source dataset
</span><span class="n">data_url</span> <span class="o">=</span> <span class="s">"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data"</span> 
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_url</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=8&amp;hideInput=true" title="Jovian Viewer" height="348" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>As shown above, the dataset doesn’t contain the column header. Here is the column list that has been created manually based on the dataset descriptions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">column</span> <span class="o">=</span> <span class="p">[</span>  <span class="s">'symboling'</span><span class="p">,</span> <span class="s">'normalized-losses'</span><span class="p">,</span> <span class="s">'make'</span><span class="p">,</span> <span class="s">'fuel-type'</span><span class="p">,</span> <span class="s">'aspiration'</span><span class="p">,</span>
            <span class="s">'num-of-doors'</span><span class="p">,</span> <span class="s">'body-style'</span><span class="p">,</span> <span class="s">'drive-wheels'</span><span class="p">,</span> <span class="s">'engine-location'</span><span class="p">,</span> <span class="s">'wheel-base'</span><span class="p">,</span>
            <span class="s">'length'</span><span class="p">,</span> <span class="s">'width'</span><span class="p">,</span> <span class="s">'height'</span><span class="p">,</span> <span class="s">'curbweight'</span><span class="p">,</span> <span class="s">'engine-type'</span><span class="p">,</span> <span class="s">'num-of-cylinders'</span><span class="p">,</span>
            <span class="s">'engine-size'</span><span class="p">,</span> <span class="s">'fuel-system'</span><span class="p">,</span> <span class="s">'bore'</span><span class="p">,</span> <span class="s">'stroke'</span><span class="p">,</span> <span class="s">'compression-ratio'</span><span class="p">,</span> <span class="s">'horsepower'</span><span class="p">,</span> 
            <span class="s">'peak-rpm'</span><span class="p">,</span> <span class="s">'city-mpg'</span><span class="p">,</span> <span class="s">'highway-mpg'</span><span class="p">,</span> <span class="s">'price'</span> <span class="p">]</span>

<span class="n">data_url</span> <span class="o">=</span> <span class="s">"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data"</span>
<span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">column</span>
<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=10&amp;hideInput=true" title="Jovian Viewer" height="362" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<h3 id="data-cleaning">Data Cleaning</h3>

<ul>
  <li>Check the null value.
<br />
The null values denoted by ‘?’ mark.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">isin</span><span class="p">([</span><span class="s">'?'</span><span class="p">]).</span><span class="nb">sum</span><span class="p">())</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=13&amp;hideInput=true" title="Jovian Viewer" height="731" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Handle the missing values.
<br />
All the null values will be removed, starting with normalized-losses column (otherwise all 40 rows will be removed) along with the symboling column because there is no enough information about this feature. For the rest of the missing values, replace ‘?’ mark with a null value (numpy.nan) then perform dropna().</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'normalized-losses'</span><span class="p">,</span><span class="s">'symboling'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'?'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=15&amp;hideInput=true" title="Jovian Viewer" height="362" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(192, 24)
</code></pre></div></div>

<p><br />
After the null values were removed, now the dataset consist of 192 rows and 24 features (columns).
<br /></p>
<ul>
  <li>Check the data type of df dataset columns.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>
<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=19&amp;hideInput=true" title="Jovian Viewer" height="800" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>The df.info() shows that there are some columns hasn’t have the correct data type.
<br /></p>
<ul>
  <li>Convert the columns datatype.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># to float
</span><span class="n">to_float</span> <span class="o">=</span> <span class="p">[</span><span class="s">'bore'</span><span class="p">,</span> <span class="s">'stroke'</span><span class="p">,</span> <span class="s">'horsepower'</span><span class="p">,</span> <span class="s">'peak-rpm'</span><span class="p">,</span> <span class="s">'price'</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">to_float</span><span class="p">:</span>
    <span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    
<span class="n">df</span><span class="p">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>
<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=22&amp;hideInput=true" title="Jovian Viewer" height="800" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>Now, the columns have the correct data type and it’s ready for some calculations.
<br /></p>
<ul>
  <li>Histogram of the Car Price.
<br />
The value that will be predicted (Price) is called the Target or Dependent Variable and the predictor is called the Features or Independent Variables.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">displot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'price'</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Car Price Distribution Data'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=25&amp;hideInput=true" title="Jovian Viewer" height="440" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>As shown in the plot above, most of the car prices are concentrated between the range 5000 to 20000 and the data has right-skewed distribution.
<br /></p>
<ul>
  <li>Price range based on categorical variables.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cat_var</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="o">!=</span><span class="s">'make'</span><span class="p">].</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'object'</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="c1"># enumerate(df.columns): return list of tuple(index column, name of column)
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cat_var</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
    
    <span class="c1"># 3 rows, 3 columns, index enumerate() + 1
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> 
    
    <span class="c1"># x: name of the column
</span>    <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>  <span class="n">x</span><span class="o">=</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="s">'price'</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s">"husl"</span><span class="p">)</span> 
    <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span> <span class="o">=</span> <span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=28&amp;hideInput=true" title="Jovian Viewer" height="481" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>There are some outliers data based on the plots, but it’s permissible for several reasons. Their occurrence is natural and the error can’t be identified or corrected. So, in this step, the outliers won’t be removed.
<br /></p>
<ul>
  <li>Summary of the numerical value.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_var</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="o">!=</span><span class="s">'make'</span><span class="p">].</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'float'</span><span class="p">,</span><span class="s">'int64'</span><span class="p">])</span>
<span class="n">num_var</span><span class="p">.</span><span class="n">describe</span><span class="p">()</span> <span class="c1"># or df.describe
</span></code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=31&amp;hideInput=true" title="Jovian Viewer" height="355" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Plot a heatmap based on the correlation value of each numerical variables.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">num_var</span><span class="p">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Blues'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Correlation Heatmap of Numerical Variables'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=33&amp;hideInput=true" title="Jovian Viewer" height="544" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>Based on the heatmap, several variables have strong positive or negative correlations. In the simple linear regression analysis, engine-size will be used as the predictor. Then, all the categorical and numerical variables that have been explored before, will be used in the multiple linear regression analysis as well.
<br /></p>
<h1 id="simple-linear-regression">Simple Linear Regression</h1>

<ul>
  <li>Scatter plot and histogram (joint plot) of engine-size vs price.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'engine-size'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'price'</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">'reg'</span><span class="p">).</span><span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">"Scatter Plot &amp; Histogram of Engine Size vs. Price"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=37&amp;hideInput=true" title="Jovian Viewer" height="498" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Duplicate df dataframe and randomly return the sample of the dataframe.
<br />
Parameter frac=1 means 100% of the dataframe will return randomly and the random_state parameter is for reproducibility (similar to random seed in numpy).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_slr</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># slr: simple linear regression
</span><span class="n">df_slr</span> <span class="o">=</span> <span class="n">df_slr</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_slr</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=39&amp;hideInput=true" title="Jovian Viewer" height="334" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Assign engine-size as the feature (x) and price as the target (y).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_slr</span> <span class="o">=</span> <span class="n">df_slr</span><span class="p">[</span><span class="s">'engine-size'</span><span class="p">]</span>
<span class="n">y_slr</span> <span class="o">=</span> <span class="n">df_slr</span><span class="p">[</span><span class="s">'price'</span><span class="p">]</span>
</code></pre></div></div>
<p><br /></p>
<ul>
  <li>Add constant (represent of intercept) to x variable (where y = a + bx).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_slr</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x_slr</span><span class="p">)</span>
<span class="n">x_slr</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=43&amp;hideInput=true" title="Jovian Viewer" height="243" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Split data into a training set and testing set.
<br />
Split 75% of the data into a training set and 25% for testing the model. Scikit-learn library has the same function for this kind of task (model_selection.train_test_split).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.75</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_slr</span><span class="p">))</span>

<span class="n">x_train_slr</span> <span class="o">=</span> <span class="n">x_slr</span><span class="p">[:</span><span class="n">train_size</span><span class="p">]</span>
<span class="n">y_train_slr</span> <span class="o">=</span> <span class="n">y_slr</span><span class="p">[:</span><span class="n">train_size</span><span class="p">]</span>

<span class="n">x_test_slr</span> <span class="o">=</span> <span class="n">x_slr</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
<span class="n">y_test_slr</span> <span class="o">=</span> <span class="n">y_slr</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>

<span class="n">x_train_slr</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test_slr</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((144, 2), (48, 2))
</code></pre></div></div>

<p><br /></p>
<ul>
  <li>Fit linear regression model and view the summary.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lm_slr</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_train_slr</span><span class="p">,</span> <span class="n">x_train_slr</span><span class="p">).</span><span class="n">fit</span><span class="p">()</span>

<span class="n">lm_slr</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=47&amp;hideInput=true" title="Jovian Viewer" height="585" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Plot the linear regression model.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">graphics</span><span class="p">.</span><span class="n">plot_regress_exog</span><span class="p">(</span><span class="n">lm_slr</span><span class="p">,</span> <span class="s">'engine-size'</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=50&amp;hideInput=true" title="Jovian Viewer" height="513" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>The statsmodels’ plot_regress_exog function allows for viewing regression results against a single regressor, which in this case is enginesize. Four different plots are generated by this function:</p>

<ul>
  <li>The upper-left (‘Y and Fitted vs. X’) plot displays the fitted values of the regression line (in red) versus the actual values of enginesize and price, with vertical lines representing prediction confidence intervals for each fitted value.</li>
  <li>The second plot, showing the residuals of the regression versus the predictor variable (enginesize), can help identify any non-linear patterns. If residuals are evenly spread out around the 0 line, it indicates that the regression model does not have any non-linear patterns.</li>
  <li>The Partial regression plot is used to demonstrate the effect of adding an independent variable to a model that already has one or more independent variables. As this is a single-variable model, the Partial regression plot simply displays a scatter plot of price versus horsepower with a fitted regression line.</li>
  <li>Lastly, the CCPR (Component-Component Plus Residual) plot allows for assessing the impact of one regressor (enginesize) on the response variable (price) while accounting for the effects of other independent variables. In this case, as there are no other independent variables in this regression, the plot simply shows a scatter plot with a linear model fit on the data.</li>
</ul>

<p><br /></p>
<ul>
  <li>Influence plot of the linear regression model.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sm</span><span class="p">.</span><span class="n">graphics</span><span class="p">.</span><span class="n">influence_plot</span><span class="p">(</span><span class="n">lm_slr</span><span class="p">)</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=53&amp;hideInput=true" title="Jovian Viewer" height="663" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>The influence_plot can be utilized to gain a deeper understanding of the regression model. This plot enables the identification of records in the dataset that have had a significant influence on the regression analysis. The influential data points can be recognized by their large circles in the plot. For example, the data point with ID 103 has a significant impact on the regression results.
<br /></p>
<ul>
  <li>Predict the testing data using the model.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_slr</span> <span class="o">=</span> <span class="n">lm_slr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_slr</span><span class="p">)</span>

<span class="n">y_pred_slr</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=56&amp;hideInput=true" title="Jovian Viewer" height="185" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Plot the the test value and the corresponding predicted value.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_test_slr</span><span class="p">[</span><span class="s">'engine-size'</span><span class="p">],</span> <span class="n">y_test_slr</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test_slr</span><span class="p">[</span><span class="s">'engine-size'</span><span class="p">],</span> <span class="n">y_pred_slr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Engine Size'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Price'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=58&amp;hideInput=true" title="Jovian Viewer" height="473" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>The fitted line of the predicted values can be seen here. Here is a scatter plot of the test data (engine size vs. price) and the matching predicted value for each x value in the test data. It appears to have quite accurately approximated or fitted the test data.</p>

<p><br /></p>
<h1 id="multiple-linear-regression">Multiple Linear Regression</h1>

<ul>
  <li>Duplicate the df dataframe and show the columns of the duplicated dataframe.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_mlr</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># mlr: multiple linear regression
</span><span class="n">df_mlr</span><span class="p">.</span><span class="n">columns</span>
</code></pre></div></div>
<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=64&amp;hideInput=true" title="Jovian Viewer" height="185" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>
<p><br /></p>

<ul>
  <li>Create a list that contains only two unique values and perform the label encoding.
<br />
Some of the variables in this linear regression analysis are the pre-determined values that are categorical. So to use these variables as predictors, it has to be encoded or converted to numeric values in binary form. It will be using the LabelEncoder from the scikit‑learn library. To every discrete value that these variables take on, the label and quota will assign a unique integral value. For example, gas might be 0, and fuel-type diesel will be 1.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'fuel-type'</span><span class="p">,</span><span class="s">'aspiration'</span><span class="p">,</span><span class="s">'num-of-doors'</span><span class="p">,</span><span class="s">'engine-location'</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cols</span><span class="p">:</span>
    <span class="n">df_mlr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_mlr</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="c1"># similar to DataFrame.head() but it returning the data randomly    
</span><span class="n">df_mlr</span><span class="p">[</span><span class="n">cols</span><span class="p">].</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> 
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=66&amp;hideInput=true" title="Jovian Viewer" height="243" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>
<p><br /></p>

<ul>
  <li>Perform ‘one-hot’ encoding to other categorical variables.
<br />
One‑hot encoding is a converting process to represent categorical variables in numeric form (such as the previous one). One‑hot encoding will replace the original column with a new column, one corresponding to each category value. So there will be a column for convertible, a column for sedan, a column for hatchback, and so on. A value of 1 will indicate that the car belongs to that category. A value of 0 indicates the car does not belong to a category. This one‑hot encoding will be using pd.get_dummies. Then remove the original columns using the DataFrame.drop function.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_mlr</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'make'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">cat_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'body-style'</span><span class="p">,</span> <span class="s">'engine-type'</span><span class="p">,</span> <span class="s">'drive-wheels'</span><span class="p">,</span> <span class="s">'num-of-cylinders'</span><span class="p">,</span> <span class="s">'fuel-system'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cat_columns</span><span class="p">:</span>
    <span class="n">df_mlr</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_mlr</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                        <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df_mlr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                        <span class="n">prefix</span> <span class="o">=</span> <span class="n">i</span><span class="p">,</span>
                                        <span class="n">prefix_sep</span> <span class="o">=</span> <span class="s">'_'</span><span class="p">,</span>
                                        <span class="n">drop_first</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">df_mlr</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=68&amp;hideInput=true" title="Jovian Viewer" height="334" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get the number of rows and columns
</span><span class="n">df_mlr</span><span class="p">.</span><span class="n">shape</span> 
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(192, 39)
</code></pre></div></div>

<p><br /></p>

<ul>
  <li>Duplicate the dataframe, assign it into features and target, then add constant.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># copy the dataframe
</span><span class="n">df_mlr</span> <span class="o">=</span> <span class="n">df_mlr</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># assign df_mlr into the x variables (exclude price variable)
</span><span class="n">x_mlr</span> <span class="o">=</span> <span class="n">df_mlr</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'price'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># assign df_mlr['price'] as the target
</span><span class="n">y_mlr</span> <span class="o">=</span> <span class="n">df_mlr</span><span class="p">[</span><span class="s">'price'</span><span class="p">]</span> 

<span class="c1"># add constant
</span><span class="n">x_mlr</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x_mlr</span><span class="p">)</span> 

<span class="n">x_mlr</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=71&amp;hideInput=true" title="Jovian Viewer" height="334" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p><br /></p>
<ul>
  <li>Split the features and the target into train and test set.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.75</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_mlr</span><span class="p">))</span>

<span class="n">x_train_mlr</span> <span class="o">=</span> <span class="n">x_mlr</span><span class="p">[:</span><span class="n">train_size</span><span class="p">]</span>
<span class="n">y_train_mlr</span> <span class="o">=</span> <span class="n">y_mlr</span><span class="p">[:</span><span class="n">train_size</span><span class="p">]</span>

<span class="n">x_test_mlr</span> <span class="o">=</span> <span class="n">x_mlr</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
<span class="n">y_test_mlr</span> <span class="o">=</span> <span class="n">y_mlr</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
</code></pre></div></div>
<p><br /></p>
<ul>
  <li>Fit the linear model.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lm_mlr</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_train_mlr</span><span class="p">,</span> <span class="n">x_train_mlr</span><span class="p">).</span><span class="n">fit</span><span class="p">()</span>

<span class="n">lm_mlr</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=75&amp;hideInput=true" title="Jovian Viewer" height="800" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<ul>
  <li>Predicting the testing data.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mlr</span> <span class="o">=</span> <span class="n">lm_mlr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_mlr</span><span class="p">)</span>

<span class="n">y_pred_mlr</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=78&amp;hideInput=true" title="Jovian Viewer" height="263" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>
<p><br /></p>

<ul>
  <li>Create a dataframe to plot the test target values and the predicted values.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_actual_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'Actual Value'</span> <span class="p">:</span> <span class="n">y_test_mlr</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span>
                                 <span class="s">'Predicted Value'</span> <span class="p">:</span> <span class="n">y_pred_mlr</span><span class="p">})</span>

<span class="n">data_actual_pred</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=80&amp;hideInput=true" title="Jovian Viewer" height="243" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>
<p><br /></p>

<ul>
  <li>Transform (unpivot) data_actual_pred dataframe into a suitable form for plotting.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">melted_data_actual_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">melt</span><span class="p">(</span><span class="n">data_actual_pred</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(),</span>
                                   <span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s">'index'</span><span class="p">],</span>
                                   <span class="n">value_vars</span><span class="o">=</span><span class="p">[</span><span class="s">'Actual Value'</span><span class="p">,</span> <span class="s">'Predicted Value'</span><span class="p">])</span>
</code></pre></div></div>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=82&amp;hideInput=true" title="Jovian Viewer" height="243" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>
<p><br /></p>

<ul>
  <li>Create a comparison lineplot of the actual value vs. the predicted value.
```python
plt.figure(figsize=(12,8))
sns.lineplot(data=melted_data_actual_pred, x=’index’, y=’value’, hue=’variable’)</li>
</ul>

<p>plt.show()
```</p>

<iframe src="https://jovian.com/embed?url=https://jovian.com/mohammadagus1st/linear-regression-ordinary-least-square-using-python-statsmodels-ipynb/v/1&amp;cellId=84&amp;hideInput=true" title="Jovian Viewer" height="473" width="100%" style="margin 0 auto; max-width: 800px;" frameborder="0" scrolling="auto"></iframe>

<p>The plot shows the actual and the predicted values are close. It indicates this is a good model.</p>

<h1 id="conclusions">Conclusions</h1>
<h2 id="simple-linear-regression-1">Simple Linear Regression</h2>
<ul>
  <li>The model’s R-squared is 0.792 or 79.2%, indicating that the linear model is reasonably effective in predicting price, given that it utilizes only one feature. This value signifies that the model captured 79.2% of the variance in the underlying data.</li>
  <li>The F-statistic and corresponding P-value evaluate the validity of the regression analysis in its entirety. A P-value less than 5% indicates the analysis is valid. The null hypothesis for this test is that all regression coefficients equal zero, which is not the case in this scenario. Therefore, the null hypothesis can be rejected, and the alternative hypothesis that regression coefficients are not equal to zero can be accepted.</li>
  <li>For each regression coefficient, a P-statistic and corresponding P-value quantify the precision and validity of the coefficient. The P-values of 0.000 for the intercept and 0.000 for the enginesize coefficient demonstrate their validity. The T-statistic and P-value confirm the statistical significance of the relationship between enginesize and price. In this simple linear regression, the coefficient of engine-size predictor is 181.8057, which means a 1 unit increase in engine-size will be adding 181.8057 to the car price.
    <h2 id="multiple-linear-regression-1">Multiple Linear Regression</h2>
  </li>
  <li>The R‑squared score has increased from 0.792 to 0.956 (95.6%) and the adjusted R-squared score is 0.941. In multiple linear regression, it’s necessary to evaluate the adjusted R-squared because not all the predictors are relevant and the adjusted R-squared applies penalty calculations to the irrelevant variables that are included in the regression analysis. The score of R-squared and the adjusted R-squared is slightly different, this indicates there is an irrelevant predictor in this model. Below the adjusted R-squared there are the F-statistics and the corresponding p-value for the analysis. The p‑value is under the significant threshold of 5% indicating that this is a valid regression analysis.</li>
  <li>Each predictor in this model has a coefficient of regression, t-statistic, and p-value that indicates the validity of that regression coefficient. Take a look at the p-value of engine-size coefficient which is 0. The null hypothesis for this statistical test is that this coefficient has no impact or effect on the regression. The alternative hypothesis is that this coefficient has an impact or effect on the regression. With the p‑value of 0, the alternative hypothesis is accepted if the p-value is under the significance threshold (0.05 or 5%). All the predictor’s coefficient that has the corresponding p-value above the significance threshold doesn’t have effects on this regression, so it’s considered irrelevant predictors in this linear regression model.</li>
</ul>

<p>Lastly, according to Chin (1998), the R-squared score that more than 0.67 is categorized as a substantial. Therefore, the closer the R-squared value is to 1, the better the fit of the model.</p>

<h1 id="credits">Credits</h1>
<ul>
  <li>Janani Ravi, for providing a clear and insightful explanation of linear regression in the course “Foundations of Statistics and Probability for Machine Learning” on <a href="https://app.pluralsight.com/library/courses/foundations-statistics-probability-machine-learning/table-of-contents">Pluralsight</a>. Janani Ravi’s explanation was a valuable resource in the development of this article.</li>
  <li>Chin, W. W. (1998). <a href="https://www.researchgate.net/publication/311766005_The_Partial_Least_Squares_Approach_to_Structural_Equation_Modeling">The Partial Least Squares Aproach to Structural Equation Modeling. Modern Methods for Business Research</a>, 295, 336</li>
</ul>


</div>

<div class="pagination">
  
    <a href="/2023-03-20/analyzing-unicorn-companies-sql-project" class="left next">Prev</a>
  
  
    <a href="/2023-02-16/handling_outliers_and_missing_values_in_python" class="right next">Next</a>
  

  <a href="#" class="top">Top</a>
</div>

    </main>

    <footer>
      <span>
        &copy; <time datetime="2023-06-05 21:37:13 +0800">2023</time> <!--Moh. Agus. <a href="https://github.com/kssim/about-portfolio/">A.P</a> theme by kssim.-->
      </span>
    </footer>
  </body>
</html>
